% !TEX root = ../Thesis.tex

\chapter{Implementierung und Ergebnisse}
\index{Implementierung}
\index{Benchmark}
\index{Evaluation}
\label{ch:realisierung}

\section{Implementierungsdetails}

In diesem Kapitel werden die praktischen Aspekte der Umsetzung dokumentiert\index{Implementierungspraxis}.\footnote{Implementierungsdetails und Best Practices sind in \cite{doe2019research} dokumentiert.} Die Implementierung erfolgte iterativ\index{Iterative Entwicklung} über einen Zeitraum von 4 Monaten mit kontinuierlichem Testen\index{Continuous Testing} und Refinement.

\subsection{Technologiestack}

Folgende Technologien wurden für die Implementierung eingesetzt:

\begin{description}
  \item[Programmiersprache:] Python 3.11+\index{Python} (Type Hints\index{Type Hints}, async/await\index{Async/Await} Support)
  \item[LLM-Integration:] OpenAI API (GPT-4)\index{GPT-4}, Anthropic API (Claude 3.5 Sonnet)\index{Claude}, mit Fallback-Mechanismus\index{Fallback-Mechanismus}
  \item[Orchestrierung:] LangChain 0.1.x für Basis-Komponenten\index{LangChain}, custom Extensions für SE-specific Tools; strukturierte Tool-Calls über \textbf{JSON}-Schemas und Konfigurationen per \textbf{YAML}.
  \item[Gedächtnis:] Chroma\index{Chroma Vector-DB} Vector-DB für semantisches Retrieval, SQLite für episodische Logs\index{Event-Logging}
  \item[Tool-Runtime:] Docker 24.0+\index{Docker!Sandboxing} für Sandboxing, pytest für Tests, ruff/black für Linting\index{Code Linting}
  \item[Monitoring:] Prometheus für Metriken\index{Prometheus}, strukturierte Logs (JSON) mit Trace-IDs\index{Distributed Tracing}
\end{description}

\subsection{Projektstruktur}

Das Projekt folgt einer modularen Architektur:

\begin{verbatim}
agent_se/
+-- core/
|   +-- agent.py           # Agent Controller (ReAct-Loop)
|   +-- policy.py          # Planning & Reflection Logic
|   +-- state.py           # State Management
+-- tools/
|   +-- base.py            # Tool Interface & Registry
|   +-- code_tools.py      # Linter, Formatter, AST-Parser
|   +-- test_tools.py      # pytest, coverage, Test-Runner
|   +-- vcs_tools.py       # git operations
+-- memory/
|   +-- episodic.py        # Event Logging & Retrieval
|   +-- semantic.py        # Vector Store Integration
+-- safety/
|   +-- validator.py       # Input Validation & Sanitization
|   +-- sandbox.py         # Docker-based Execution Env
+-- evaluation/
    +-- benchmarks.py      # Test Scenarios
    +-- metrics.py         # Success Rate, Costs, Latency
\end{verbatim}

\subsection{Kernimplementierung: Agent Controller}\index{Agent Controller!Implementierung}

Listing \ref{lst:agent-loop} zeigt einen minimalen agentischen Loop\index{Agent Loop} mit Planung\index{Planung!Implementierung}, Tool-Ausführung und Reflexion\index{Reflexion}. Die vollständige Implementierung umfasst zusätzlich Error-Handling\index{Error-Handling!Agent}, Timeouts, max-steps-Limits und strukturiertes Logging\index{Agent Logging}.

\begin{lstlisting}[style=Python, caption={Minimaler agentischer Loop fuer SE-Aufgaben (Beispiel-Listing)}, label={lst:agent-loop}]
from typing import Dict, Any

class Toolset:
  def run_tests(self) -> str:
    return "tests: 103 passed, 2 failed"

  def format_code(self, diff: str) -> str:
    return "formatted diff applied"

class Agent:
  def __init__(self, tools: Toolset):
    self.tools = tools
    self.memory = []  # episodic traces

  def plan(self, goal: str) -> str:
    return f"Plan: run tests -> fix failures -> re-run -> format -> commit ({goal})"

  def act(self, step: str) -> str:
    if "run tests" in step:
      return self.tools.run_tests()
    if "format" in step:
      return self.tools.format_code(diff="...")
    return "noop"

  def reflect(self, observation: str) -> str:
    if "failed" in observation:
      return "Next: inspect failing tests and patch code"
    return "Next: finalize and commit"

  def run(self, goal: str) -> Dict[str, Any]:
    plan = self.plan(goal)
    self.memory.append({"plan": plan})
    obs = self.act("run tests")
    self.memory.append({"obs": obs})
    next_step = self.reflect(obs)
    self.memory.append({"reflect": next_step})
    obs2 = self.act("format")
    self.memory.append({"obs": obs2})
    return {"status": "done", "trace": self.memory}

agent = Agent(Toolset())
result = agent.run(goal="increase reliability of module X")
print(result["status"])
\end{lstlisting}

% ------------------------------------------------------------
% Zweites Listing: TypeScript Tool-Calling Stub
\begin{lstlisting}[style=TypeScript, caption={Tool-Calling Stub in TypeScript mit einfachem Funktionsschema (Beispiel-Listing)}, label={lst:ts-tool-calling}]
type ToolName = "run_tests" | "format_code" | "open_issue";

interface ToolCall {
  name: ToolName;
  args: Record<string, unknown>;
}

interface ToolResult {
  name: ToolName;
  ok: boolean;
  output: string;
}

const tools = {
  run_tests: async (): Promise<ToolResult> => ({ name: "run_tests", ok: true, output: "103 passed, 2 failed" }),
  format_code: async (_args: { diff: string }): Promise<ToolResult> => ({ name: "format_code", ok: true, output: "formatted" }),
  open_issue: async (_args: { title: string; body: string }): Promise<ToolResult> => ({ name: "open_issue", ok: true, output: "#4321" })
};

async function dispatch(call: ToolCall): Promise<ToolResult> {
  switch (call.name) {
    case "run_tests":
      return tools.run_tests();
    case "format_code":
      return tools.format_code(call.args as { diff: string });
    case "open_issue":
      return tools.open_issue(call.args as { title: string; body: string });
  }
}

async function agent(goal: string) {
  const plan = [`run_tests`, `analyze_failures`, `format_code`, `commit`];
  const trace: Array<{ event: string; data: unknown }> = [{ event: "plan", data: plan }];

  const res1 = await dispatch({ name: "run_tests", args: {} });
  trace.push({ event: "tool_result", data: res1 });

  if (res1.output.includes("failed")) {
    // Simple reflection -> open an issue with details
    const res2 = await dispatch({ name: "open_issue", args: { title: `Test failures for ${goal}`, body: res1.output } });
    trace.push({ event: "tool_result", data: res2 });
  }

  const res3 = await dispatch({ name: "format_code", args: { diff: "..." } });
  trace.push({ event: "tool_result", data: res3 });
  return { status: "done", trace };
}

agent("increase reliability of module X").then(r => console.log(r.status));
\end{lstlisting}

\section{Experimentelles Setup}

Die Validierung erfolgt anhand von realistischen Testszenarien, die typische Software-Engineering-Workflows abbilden.

\subsection{Testumgebung}

\begin{description}
  \item[Hardware:] MacBook Pro M2, 16GB RAM, 512GB SSD
  \item[Betriebssystem:] macOS 14.x, Docker Desktop 4.25
  \item[LLM-Modelle:] GPT-4-Turbo (0125), Claude-3.5-Sonnet, mit Temperature 0.2 für Reproduzierbarkeit
  \item[Testdaten:] 25 repräsentative Python-Projekte (5k--50k LOC), synthetische Bugs, real-world Issues
\end{description}

\subsection{Benchmark-Szenarien}

Drei Haupt-Szenarien wurden evaluiert (vgl. Kapitel \ref{ch:konzept}):

\begin{enumerate}
  \item \textbf{Automated Refactoring} (10 Tasks): Extract-Function, Rename-Variable, Simplify-Conditional
  \item \textbf{Test Failure Diagnosis} (8 Tasks): Debug failing tests, fix assertions, update mocks
  \item \textbf{Lint Error Resolution} (7 Tasks): Fix style issues, type errors, unused imports
\end{enumerate}

Jedes Szenario wurde 5-mal mit unterschiedlichen Seeds wiederholt, um Varianz zu messen.

\subsection{Beispiel: Test Failure Diagnosis — Schritt-für-Schritt}

Im Folgenden wird das Szenario \enquote{Test Failure Diagnosis} detailliert beschrieben, um den praktischen Ablauf und die typischen Artefakte (Tool-Calls, Logs, Entscheidungen) zu veranschaulichen.

1) Initialer Zustand: Test-Runner meldet \texttt{3 failed} in \texttt{auth/test\_login.py}. Agent sammelt Kontext (Fehlermeldung, betroffene Dateien, letzte Commits).

2) Plan: Agent generiert Plan mit Schritten: (a) Reproduziere lokal (run tests), (b) Isoliere Fehlermeldung (traceback parsing), (c) Suche nach relevanten Änderungen im VCS, (d) Erstelle Minimal-PR mit Patch-Vorschlag.

3) Act: Tool-Calls (Beispiel):
\begin{itemize}
  \item \texttt{run\\_tests(module="auth")} $\to$ Tool-Result: \texttt{"3 failed, 97 passed"}
  \item \texttt{run\_linter(path="auth/")} $\to$ Tool-Result: Hinweise auf Stil, aber keine direkten Fehler
  \item \texttt{search\_in\_repo(query="login", range="last-5-commits")} $\to$ Treffer: Commit 123abc \texttt{Refactor: auth flow}
\end{itemize}

4) Observation: Agent parst Traceback, findet NullPointer-ähnlichen Fehler in Helper-Funktion, die neu extrahiert wurde. Memory-Manager liefert ähnlichen früheren Fall (Match on signature), Agent übernimmt learnings aus historischem Patch.

5) Reflection 
\begin{itemize}
  \item Agent generiert Patch-Vorschlag (kleine Scope-Fix im Helper), erstellt Diff und führt Tests erneut in isolierter Sandbox aus.
  \item Tests grün $\Rightarrow$ Agent eröffnet PR-Entwurf und notiert Review-Kommentare; bei Teil-Erfolg: Human-Approval-Gate vor Merge.
\end{itemize}

Beispiel-Trace (gekürzt):
\begin{verbatim}
[Agent] run_tests -> 3 failed (auth/test_login.py::test_login)
[Agent] search_in_repo -> found commit 123abc (Refactor auth flow)
[Agent] generate_patch -> diff created: modify auth/helpers.py
[Agent] run_tests (sandbox) -> 100 passed
[Agent] open_pr -> PR #42 (Draft)
\end{verbatim}

Dieses Beispiel zeigt, wie Tool-Adapter, Memory und Safety-Layer (Sandbox, Human-Approval) zusammenwirken, um fehlerhafte Änderungen sicher zu erkennen und zu beheben. Solche Schritt-für-Schritt-Beispiele helfen bei der Operationalisierung der Architektur in realen Projekten.

\section{Ergebnisse}

Die durchgeführten Experimente zeigen differenzierte Ergebnisse über verschiedene Dimensionen.

\subsection{Erfolgsraten nach Aufgabentyp}

Tabelle \ref{tab:benchmark-results} zeigt detaillierte Ergebnisse für ausgewählte Tasks:

\begin{itemize}
  \item \textbf{Refactoring:} \pct{73} Erfolgsrate (11/15 Tasks erfolgreich)
  \item \textbf{Test-Fixing:} \pct{62} Erfolgsrate (5/8 Tasks erfolgreich)
  \item \textbf{Lint-Resolution:} \pct{86} Erfolgsrate (6/7 Tasks erfolgreich)
\end{itemize}

Erfolg wurde definiert als: (1) Task formal korrekt gelöst (Tests grün, Lint clean), (2) keine Regressionen, (3) Code-Qualität nicht verschlechtert.

\subsection{Effizienzmetriken}

Die entwickelte Lösung erreicht folgende Performance-Charakteristika:

\begin{description}
  \item[Token-Verbrauch:] Durchschnittlich 8.4k tokens pro erfolgreichem Task (Range: 2k--25k). Kontextoptimierung reduzierte Verbrauch um \pct{40} gegenüber naivem Ansatz.
  
  \item[Laufzeit:] Median 12.3 Sekunden pro Task (ohne Tool-Execution). Mit Test-Runs: 45s--180s je nach Testsuite-Größe.
  
  \item[Tool-Calls:] Durchschnittlich 4.2 Tool-Aufrufe pro Task. Reflexion reduzierte fehlerhafte Calls um \pct{28}.
  
  \item[Kosten:] Geschätzt \$0.08 pro Task bei GPT-4-Pricing (Jan 2024). Claude war \pct{35} günstiger bei vergleichbarer Qualität.
\end{description}

\subsection{Qualitätsmetriken}

\textquote[\cite{doe2019research}]{Die praktische Implementierung agentischer Systeme erfordert sorgfältige Planung und umfassende Tests, um Zuverlässigkeit in produktiven Umgebungen zu gewährleisten.}

Code-Qualität wurde über mehrere Dimensionen gemessen:

\begin{itemize}
  \item \textbf{Test-Pass-Rate:} \pct{98} (nur 2 von 103 Tests regressierten nach Agent-Edits)
  \item \textbf{Lint-Score:} Durchschnittlich +12 Punkte Verbesserung nach Lint-Fixes
  \item \textbf{Complexity:} Cyclomatic Complexity blieb unverändert oder verbesserte sich (Extract-Function Tasks)
  \item \textbf{Review-Akzeptanz:} Manuelle Review ergab \pct{81} „would merge" Rate
\end{itemize}

\subsection{Robustheit und Fehlerbehandlung}

Error-Cases wurden systematisch getestet:

\begin{itemize}
  \item \textbf{Tool-Failures:} Agent erholte sich in \pct{67} der Fälle durch Retry oder Alternative-Strategie
  \item \textbf{Malformed Outputs:} JSON-Parsing-Fehler wurden durch Retry-mit-Schema-Reinforcement in \pct{89} behoben
  \item \textbf{Timeouts:} Graceful Degradation bei max-steps Limit (definiert als \pct{15} Partial-Success)
\end{itemize}

\section{Vergleich mit existierenden Ansätzen}

Die entwickelte Lösung wurde mit Baselines verglichen:

\begin{table}[ht]
\centering
\caption{Vergleich mit Baseline-Systemen (auf gleichem Benchmark-Set)}
\label{tab:comparison}
\begin{tabular}{L{3cm}C{2cm}C{2cm}C{2.5cm}C{2cm}}
\toprule
\cellcolor{headercolor}\textcolor{white}{\textbf{Ansatz}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Success}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Token}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Zeit (s)}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Kosten}} \\
\midrule
Naive Prompting & \pct{42} & 12.3k & 8.5 & \$0.12 \\
\rowcolor{rowcolorgray}
ReAct (Baseline) & \pct{58} & 10.1k & 15.2 & \$0.10 \\
Unsere Architektur & \pct{73} & 8.4k & 12.3 & \$0.08 \\
\rowcolor{rowcolorgray}
+ Reflexion & \pct{73} & 8.9k & 14.1 & \$0.09 \\
\bottomrule
\end{tabular}
\end{table}

Kernverbesserungen gegenüber Baselines:

\begin{itemize}
  \item \textbf{+31\% Erfolgsrate} vs. Naive Prompting durch strukturierte Tool-Orchestrierung
  \item \textbf{+15\% Erfolgsrate} vs. Standard-ReAct durch optimierte Context-Management
  \item \textbf{-17\% Token-Kosten} durch intelligentes Pruning und Summarization
  \item \textbf{Robustere Error-Recovery} durch Reflexions-Mechanismen
\end{itemize}

Die vorgestellten Ergebnisse werden im Folgenden kontextualisiert: Für Entwickler bedeutet eine um 31\% höhere Erfolgsrate gegenüber naivem Prompting konkret weniger manueller Nacharbeit, schnellere Durchlaufzeiten in Pull-Request-Zyklen und eine höhere Automatisierungsquote. Für Betreiber von CI/CD-Infrastrukturen bedeuten niedrigere Token-Kosten und reduzierte Fehlerraten geringere Betriebskosten. Diese praktische Perspektive ist wichtig, um Entscheidungsträgern in Unternehmen handfeste Argumente für den Einsatz agentischer Lösungen zu liefern.

\section{Validierung und Verifikation}

Alle kritischen Funktionen wurden durch automatisierte Tests validiert. Die Testabdeckung beträgt \pct{87} (Zeilen-Coverage).

\subsection{Unit-Tests}

\begin{itemize}
  \item Tool-Adapter: 45 Tests, \pct{95} Coverage
  \item Policy-Logic: 32 Tests, \pct{89} Coverage  
  \item Safety-Layer: 28 Tests, \pct{92} Coverage
\end{itemize}

\subsection{Integration-Tests}

End-to-End-Tests für alle 3 Benchmark-Szenarien. Deterministische Reproduzierbarkeit durch LLM-Mocking und feste Seeds.

Praktische Lessons Learned: Automatisierte Tests sollten sowohl synthetische als auch realistische Repositories umfassen; Mocking allein reicht nicht aus, da Real-World-Tests unerwartete Edge-Cases aufdecken. Darüber hinaus ist Continuous Monitoring unabdingbar, um Drift in LLM-Verhalten oder Änderungen in abhängigen Tools frühzeitig zu erkennen.

\subsection{Sicherheits-Audits}

\begin{itemize}
  \item Prompt-Injection-Tests: 15 Exploit-Versuche, alle blockiert
  \item Filesystem-Isolation: Sandbox-Escapes verhindert
  \item Rate-Limiting: Korrekte Durchsetzung bei 100 Requests/min Limit
\end{itemize}

% Benchmark-Ergebnisse mit longtable
\begin{table}[ht]
\centering
\caption{Detaillierte Benchmark-Ergebnisse: Agentische SE-Tasks mit Evaluationsmetriken}
\label{tab:benchmark-results}
\begin{tabular}{L{2.8cm}C{1.8cm}C{1.8cm}C{1.8cm}C{1.5cm}C{2cm}}
\toprule
\cellcolor{headercolor}\textcolor{white}{\textbf{Aufgabe}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Success}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Token}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Zeit (s)}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Tools}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Kosten}} \\
\midrule
Extract Function & OK & 7.2k & 18.5 & 4 & \$0.07 \\
\rowcolor{rowcolorgray}
Fix Lint Errors & OK & 3.1k & 8.2 & 3 & \$0.03 \\
Debug Test Failure & OK & 12.5k & 45.3 & 6 & \$0.12 \\
\rowcolor{rowcolorgray}
Format Code & OK & 2.8k & 5.1 & 2 & \$0.03 \\
Rename Variable & OK & 5.4k & 12.8 & 3 & \$0.05 \\
\rowcolor{rowcolorgray}
Remove Deadcode & OK & 8.9k & 22.1 & 5 & \$0.09 \\
Update Mocks & FAIL & 9.7k & 38.2 & 7 & \$0.10 \\
\rowcolor{rowcolorgray}
Simplify Conditional & OK & 6.3k & 15.7 & 4 & \$0.06 \\
Generate Docstrings & OK & 4.5k & 9.3 & 2 & \$0.04 \\
\rowcolor{rowcolorgray}
Fix Type Errors & OK & 11.2k & 28.4 & 5 & \$0.11 \\
\bottomrule
\multicolumn{6}{l}{\small \textit{Durchschnitt (erfolgreiche Tasks):} 7.1k tokens, 16.5s, 3.8 tools, \$0.07} \\
\end{tabular}
\end{table}

% Zusätzliche Beispiel-Tabelle für Evaluationsmetriken
\begin{table}[ht]
\centering
\caption{Evaluationsmetriken für agentische SE-Workflows (Beispieltabelle)}
\label{tab:eval-metrics}
\begin{tabular}{L{3.5cm}L{10.5cm}}
\toprule
\cellcolor{headercolor}\textcolor{white}{\textbf{Kategorie}} & \cellcolor{headercolor}\textcolor{white}{\textbf{Metriken / Beschreibung}} \\
\midrule
Qualität & Task-Success-Rate, Patch-Korrektheit (Tests/Lint), Review-Akzeptanz, Regressionen (\#) \\
\rowcolor{rowcolorgray}
Kosten & Token-/API-Kosten (EUR), Tool-Aufrufkosten, Compute-Zeit \\
Latenz & End-to-End-Laufzeit (s), Tool-Roundtrips (\#), Wartezeit auf CI \\
\rowcolor{rowcolorgray}
Sicherheit & Policy-Verstöße (\#), Risk Flags, sand-boxed I/O, PII-Leaks (\#) \\
Nachvollziehbarkeit & Trace-Länge (\#Events), Artefakte (Patches, Logs), Reproduzierbarkeit (Seeds) \\
\bottomrule
\end{tabular}
\end{table}


